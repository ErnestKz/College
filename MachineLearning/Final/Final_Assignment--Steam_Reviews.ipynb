{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accompanied-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_lines\n",
    "X=[]; y=[]; z=[]\n",
    "with open ('reviews_291.jl', 'rb' ) as f:\n",
    "  for item in json_lines.reader(f):\n",
    "    X.append ( item [ 'text' ] )\n",
    "    y.append ( item [ 'voted_up' ] )\n",
    "    z.append ( item [ 'early_access' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "available-spanish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      0\n",
      "3462  ÐœÐµÑ€Ñ‚Ð²Ð°Ñ Ñ‚Ð¸ÑˆÐ¸Ð½Ð°, ÑÐµÑ€Ð²ÐµÑ€Ð° Ð¿...\n",
      "2438  THIS IS â™¥â™¥â™¥â™¥â™¥â™¥â™¥ AWSOME :) MrBost...\n",
      "4763                                          uma bosta\n",
      "4683  For a buy2play game, I guess I expected more b...\n",
      "98                                                    n\n",
      "          0\n",
      "4298  False\n",
      "         0\n",
      "251  False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "text = pd.DataFrame(X)\n",
    "votes = pd.DataFrame(y)\n",
    "early = pd.DataFrame(z)\n",
    "print(text.sample(5))\n",
    "print(votes.sample())\n",
    "print(early.sample())\n",
    "text = text.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "architectural-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_x = text[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "turkish-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = []\n",
    "for sentence in subset_x:\n",
    "  lang.append(detect_language(sentence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "senior-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_copy = lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "stopped-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = []\n",
    "for message in lang_copy:\n",
    "  if message[1] == 'en':\n",
    "    eng.append(message[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "independent-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_i = []\n",
    "for i in range(len(lang_copy)):\n",
    "  if lang_copy[i][1] == 'en':\n",
    "    en_i.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "funded-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(eng, open(\"en_text.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fantastic-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(en_i, open(\"en_i.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "precious-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pickle.load(open(\"en_i.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dated-structure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "seven-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "soviet-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-buffer",
   "metadata": {},
   "source": [
    "Upon first inspection it seems that the reviews consist of \n",
    "predominantly english sentences.   \n",
    "\n",
    "There are also a considerable number of French, German, and Spanish reviews.   \n",
    "And there are also reviews that solely contain symbols. Upon encoding these symbols \n",
    "to UTF-8, they turned out to be characters of the cyrillic alphabet.   \n",
    "\n",
    "\n",
    "To predic the reviews, going to use bag of words.\n",
    "\n",
    "Three main ways to use the data.\n",
    "\n",
    "We can just make one big model with every single word included, even the \n",
    "\n",
    "An approach to be able to use all the data in the dataset and therefore predict any review would be\n",
    "to pass the data through a google transalte API. Since for this exercise we don't care about the \n",
    "gramatical structure this method might be good enough.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "angry-correspondence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relaxing game ever!..\n"
     ]
    }
   ],
   "source": [
    "sample_text = text[1][0]\n",
    "print(text[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "reverse-fantasy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i want this entire game shot directly into my veins\n",
      "['i', 'want', 'thi', 'entir', 'game', 'shot', 'directli', 'into', 'my', 'vein']\n"
     ]
    }
   ],
   "source": [
    "sample_text = text[11][0]\n",
    "print(sample_text)\n",
    "tokens = word_tokenize(sample_text)\n",
    "stems = [stemmer.stem(token) for token in tokens]\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dramatic-longitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn:\t ['Here', 'an', 'example', 'text', 'isn', 'it']\n",
      "nltk whitespace: [\"Here's\", 'an', 'example', 'text,', \"isn't\", 'it?']\n",
      "nltk word:\t ['Here', \"'s\", 'an', 'example', 'text', ',', 'is', \"n't\", 'it', '?']\n",
      "['likes', 'liking', 'liked']\n",
      "['likes', 'liking', 'liked']\n",
      "['likes', 'liking', 'liked']\n",
      "['here', \"'s\", 'exampl', 'text', ',', 'is', \"n't\", 'it', '?']\n",
      "['like', 'like', 'like']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ek/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tokenizer = CountVectorizer().build_tokenizer()\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"sklearn:\\t\", tokenizer(\"Here's an example text, isn't it?\"))\n",
    "print(\"nltk whitespace:\", WhitespaceTokenizer().tokenize(\"Here's an example text, isn't it?\"))\n",
    "print(\"nltk word:\\t\", word_tokenize(\"Here's an example text, isn't it?\"))\n",
    "\n",
    "print(tokenizer(\"likes liking liked\"))\n",
    "print(WhitespaceTokenizer().tokenize(\"likes liking liked\"))\n",
    "print(word_tokenize(\"likes liking liked\"))\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "tokens = word_tokenize(\"Here's example text, isn't it?\")\n",
    "stems = [stemmer.stem(token) for token in tokens]\n",
    "print(stems)\n",
    "tokens = word_tokenize(\"likes liking liked\")\n",
    "stems = [stemmer.stem(token) for token in tokens]\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cubic-patent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['directly', 'entire', 'game', 'into', 'my', 'shot', 'this', 'veins', 'want']\n",
      "[[1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# takes in an [ String ]\n",
    "X = vectorizer.fit_transform(text[11])\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "alone-dodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are you', 'hello my', 'how are', 'howdy how', 'my name', 'name is']\n",
      "[[0 1 0 0 1 1]\n",
      " [1 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "text_array = [\"hello my name is\", \"howdy how are you!\"]\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(text_array)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "vectorizer = CountVectorizer(stop_words=nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "national-snake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0.         1.22314355 1.51082562 1.22314355 0.         0.\n",
      "  1.         0.         1.22314355]\n",
      " [0.         1.22314355 0.         1.22314355 0.         1.91629073\n",
      "  1.         0.         1.22314355]\n",
      " [1.91629073 0.         0.         0.         1.91629073 0.\n",
      "  1.         1.91629073 0.        ]\n",
      " [0.         1.22314355 3.02165125 1.22314355 0.         0.\n",
      "  1.         0.         1.22314355]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "docs = [\n",
    "  'this is the first document', 'this is the second document', 'and the third one', 'Is this the first first document?'\n",
    "]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cutting-ocean",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-53-287241588fda>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-53-287241588fda>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    text = pd.read_csv(’articles1_1000.csv’)\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "# first 1000 articles from news dataset at https://www.kaggle.com/snapcrack/all−the−news\n",
    "text = pd.read_csv(’articles1_1000.csv’)\n",
    "text.head()\n",
    "x = text[’content’]\n",
    "vectorizer = TfidfVectorizer(stop_words = ’english’, max_df=0.2)\n",
    "X = vectorizer.fit_transform(x)\n",
    "indices = np.arange(x.from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(indices, test_size=0.2)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=3,metric=cosine_distances).fit(X[train])\n",
    "\n",
    "test=[test[0]]\n",
    "found = nbrs.kneighbors(X[test], return_distance=False)\n",
    "\n",
    "test_i=0\n",
    "print(’text:\\n%.300s’%x[test[test_i]])\n",
    "for i in found[0]:\n",
    "  print(’match %d:\\n%.300s’%(i,x[train[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "decreased-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/ek/College/MachineLearning/Final/translate-302719-4cc72fc518f9.json'\n",
    "USER = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detects the text's language.\"\"\"\n",
    "    from google.cloud import translate_v2 as translate\n",
    "    translate_client = translate.Client()\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.detect_language(text)\n",
    "    return [text, result[\"language\"], result[\"confidence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "compressed-length",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: hello\n",
      "Confidence: 1\n",
      "Language: en\n"
     ]
    }
   ],
   "source": [
    "a = detect_language(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "graduate-therapist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'en', 1]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "name": "Final_Assignment--Steam_Reviews.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
